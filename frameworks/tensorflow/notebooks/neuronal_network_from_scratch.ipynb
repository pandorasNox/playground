{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronal Network in plain python\n",
    "\n",
    "Description\n",
    "singel Layer perceptron (feedforward neural network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Language/Library versions:\n",
    "Python version:\n",
    "3.5.2 (default, Nov 23 2017, 16:37:01) \n",
    "[GCC 5.4.0 20160609]\n",
    "\n",
    "NumPy version:\n",
    "1.13.3\n",
    "\n",
    "TensorFlow version:\n",
    "1.4.1\n",
    "\n",
    "scikit-learn version:\n",
    "0.19.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "3.5.2 (default, Nov 23 2017, 16:37:01) \n",
      "[GCC 5.4.0 20160609]\n",
      "\n",
      "NumPy version:\n",
      "1.13.3\n",
      "\n",
      "TensorFlow version:\n",
      "1.4.1\n",
      "\n",
      "scikit-learn version:\n",
      "0.19.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Python version:\")\n",
    "print(sys.version)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"NumPy version:\")\n",
    "print(np.__version__)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"TensorFlow version:\")\n",
    "print(tf.__version__)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"scikit-learn version:\")\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets go\n",
    "\n",
    "# layer 0\n",
    "input1 = 0;\n",
    "w1 = 5; # how to choose the weight range?????\n",
    "b1 = 1; # how to choose the bias range?????\n",
    "input2 = 0;\n",
    "w1 = -5;\n",
    "b2 = 1;\n",
    "\n",
    "# print(input1, input2)\n",
    "\n",
    "# hidden layer 1\n",
    "# has 3 neurons\n",
    "\n",
    "# instead call it activation function?\n",
    "\n",
    "#def create_neuron(dict):\n",
    "#    return (np.random.randint(11))/10\n",
    "\n",
    "#def sigmoid(x):\n",
    "#    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "#def activation(inputs=[],w,b):\n",
    "#    sumOfWeightsMultInputs = 1+5-8;\n",
    "#    bias=5;\n",
    "#    return sigmoid(sumOfWeights + bias);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(create_neuron())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mapping = [\n",
    "    \n",
    "];\n",
    "\n",
    "0\n",
    "0\n",
    "= s(w1 * i1 + b1) = 0\n",
    "\n",
    "0\n",
    "1\n",
    "= ... = 0\n",
    "\n",
    "1\n",
    "0\n",
    "= ... = 1\n",
    "\n",
    "1\n",
    "1\n",
    "= ... = 1\n",
    "\n",
    "\n",
    "[\n",
    "    [0],\n",
    "    [0]\n",
    "]\n",
    "\n",
    "bsp:\n",
    "[\n",
    "[1,2,3],\n",
    "[4,5,6]\n",
    "]\n",
    "\n",
    "================\n",
    "\n",
    "i1=0\n",
    "i2=1\n",
    "\n",
    "#i1\n",
    "[\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],  \n",
    "]\n",
    "\n",
    "*\n",
    "\n",
    "[\n",
    "    [w0],\n",
    "    [w1],\n",
    "    [w2],\n",
    "]\n",
    "\n",
    "+\n",
    "\n",
    "#i2\n",
    "[\n",
    "    [1],\n",
    "    [1],\n",
    "    [1],  \n",
    "]\n",
    "\n",
    "*\n",
    "\n",
    "[\n",
    "    [w0],\n",
    "    [w1],\n",
    "    [w2],\n",
    "]\n",
    "\n",
    "+\n",
    "\n",
    "[\n",
    "    [b0],\n",
    "    [b1],\n",
    "    [b2]\n",
    "]\n",
    "\n",
    "=\n",
    "\n",
    "0 ... 1\n",
    "\n",
    "====================\n",
    "\n",
    "[\n",
    "    [w0*i0 + w0+i1],\n",
    "    [w1*i0 + w1+i1],\n",
    "    [w2*i0 + w2+i3],\n",
    "]\n",
    "\n",
    "+\n",
    "\n",
    "[\n",
    "    [b0],\n",
    "    [b1],\n",
    "    [b2]\n",
    "]\n",
    "\n",
    "\n",
    "====================\n",
    "\n",
    "#expect\n",
    "i0 , i1\n",
    "\n",
    "iw0, iw1\n",
    "\n",
    "lw0, lw1, lw2\n",
    "\n",
    "b0,b1,b2\n",
    "\n",
    "====\n",
    "\n",
    "Felix:\n",
    "\n",
    "inputs\n",
    "i0, i1,\n",
    "\n",
    "neuron\n",
    "n0, n1, n2\n",
    "\n",
    "bias\n",
    "b0,b1,b2\n",
    "\n",
    "weights\n",
    "wi0n0, wi0n1, wi0n2, wi1n0, wi1n1, wi1n2\n",
    "\n",
    "====\n",
    "\n",
    "3 * neuron in hidden layer 1\n",
    "\n",
    "outN0 = w0 * i0    +  w0 * i1    + b0\n",
    "\n",
    "outN1 = w1 * i0    +  w1 * i1    + b1\n",
    "\n",
    "outN0 = w2 * i0    +  w2 * i1    + b2\n",
    "\n",
    "\n",
    "=================\n",
    "\n",
    "input layer\n",
    "i00, i10\n",
    "\n",
    "weights\n",
    "w00, w01 => weight edges from input layer to 1st neuron of 1st hidden layer\n",
    "\n",
    "outN0 = w00*i00 + w01*i10 + b0\n",
    "\n",
    "sidenote\n",
    "2 inputs + 1 neuron = 2 edges\n",
    "2 inputs + 2 neuron = 4 edges\n",
    "2 inputs + 3 neuron = 6 edges\n",
    "\n",
    "weights2\n",
    "w10, w11,\n",
    "\n",
    "outN1 = w10*i00 + w11*i10 + b1\n",
    "\n",
    "weights3\n",
    "w20, w21,\n",
    "\n",
    "outN2 = w20*i00 + w21*i10 + b2\n",
    "\n",
    "consculusion\n",
    "\n",
    "outN0 = w00*i00 + w01*i10 + b0\n",
    "outN1 = w10*i00 + w11*i10 + b1\n",
    "outN2 = w20*i00 + w21*i10 + b2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siraj Raval implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "#variables\n",
    "\n",
    "n_hidden = 10\n",
    "n_in = 10\n",
    "n_out = 10\n",
    "n_samples = 300\n",
    "\n",
    "#hyperparameters\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9 #for lowering loss ?\n",
    "\n",
    "# non deterministic seeding\n",
    "np.random.seed(0) #make sure generate every time the same random data\n",
    "\n",
    "# truns numbers into probability\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "# x = input \n",
    "# t = transpose ??? helps matrics multiplication\n",
    "# V,W = layers\n",
    "# bv, bw = biases\n",
    "#\n",
    "def train(x, t, V, W, bv, bw):\n",
    "    \n",
    "    # forward propagation -- matrics multiplication + bias\n",
    "    A = np.dot(x, V) + bv\n",
    "    Z = np.tanh(A)\n",
    "\n",
    "    B = np.dot(Z, W) + bw\n",
    "    Y = sigmoid(B)\n",
    "    \n",
    "    # backward propagation\n",
    "    Ew = Y - t #errors???\n",
    "    Ev = tanh_prime(A) * np.dot(W, Ew)\n",
    "    \n",
    "    #predict our loss\n",
    "    dW = np.outer(Z, Ew)\n",
    "    dV = np.outer(x, Ev)\n",
    "    \n",
    "    #\"cross entropy\" as loss (good for classification)\n",
    "    loss = -np.mean(t * np.log(Y) + (1 - t) * np.log(1-Y))\n",
    "    loss = -np.mean ( t * np.log(Y) + (1 - t) * np.log(1 - Y) )\n",
    "    \n",
    "    return loss, (dV, dW, Ev, Ew)\n",
    "\n",
    "def predict(x, V, W, bv, bw):\n",
    "    A = np.dot(x, V) + bv\n",
    "    B = np.dot(np.tanh(A), W) + bw\n",
    "    return (sigmoid(B) > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 1 1 1 0 0 0]\n",
      "\n",
      "[0 0 1 0 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create layers\n",
    "V = np.random.normal(scale=0.1, size=(n_in, n_hidden))\n",
    "W = np.random.normal(scale=0.1, size=(n_hidden, n_out))\n",
    "\n",
    "bv = np.zeros(n_hidden);\n",
    "bw = np.zeros(n_out);\n",
    "\n",
    "params = [V, W, bv, bw]\n",
    "\n",
    "# generate our data\n",
    "X = np.random.binomial(1, 0.5, (n_samples, n_in))\n",
    "# calc transpose\n",
    "T = X ^ 1 #bitwise xor\n",
    "\n",
    "print(X[0])\n",
    "print()\n",
    "print(T[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.00121482, Time: 0.0598s\n",
      "Epoch: 1, Loss: 0.00120214, Time: 0.0313s\n",
      "Epoch: 2, Loss: 0.00118972, Time: 0.0326s\n",
      "Epoch: 3, Loss: 0.00117755, Time: 0.0347s\n",
      "Epoch: 4, Loss: 0.00116561, Time: 0.0308s\n",
      "Epoch: 5, Loss: 0.00115392, Time: 0.0302s\n",
      "Epoch: 6, Loss: 0.00114245, Time: 0.0321s\n",
      "Epoch: 7, Loss: 0.00113120, Time: 0.0459s\n",
      "Epoch: 8, Loss: 0.00112017, Time: 0.0384s\n",
      "Epoch: 9, Loss: 0.00110934, Time: 0.0352s\n",
      "Epoch: 10, Loss: 0.00109872, Time: 0.0364s\n",
      "Epoch: 11, Loss: 0.00108830, Time: 0.0346s\n",
      "Epoch: 12, Loss: 0.00107807, Time: 0.0367s\n",
      "Epoch: 13, Loss: 0.00106802, Time: 0.0466s\n",
      "Epoch: 14, Loss: 0.00105815, Time: 0.0368s\n",
      "Epoch: 15, Loss: 0.00104847, Time: 0.0282s\n",
      "Epoch: 16, Loss: 0.00103895, Time: 0.0277s\n",
      "Epoch: 17, Loss: 0.00102960, Time: 0.0295s\n",
      "Epoch: 18, Loss: 0.00102042, Time: 0.0303s\n",
      "Epoch: 19, Loss: 0.00101139, Time: 0.0301s\n",
      "Epoch: 20, Loss: 0.00100252, Time: 0.0437s\n",
      "Epoch: 21, Loss: 0.00099380, Time: 0.0346s\n",
      "Epoch: 22, Loss: 0.00098523, Time: 0.0291s\n",
      "Epoch: 23, Loss: 0.00097680, Time: 0.0292s\n",
      "Epoch: 24, Loss: 0.00096851, Time: 0.0286s\n",
      "Epoch: 25, Loss: 0.00096036, Time: 0.0354s\n",
      "Epoch: 26, Loss: 0.00095234, Time: 0.1398s\n",
      "Epoch: 27, Loss: 0.00094445, Time: 0.0775s\n",
      "Epoch: 28, Loss: 0.00093668, Time: 0.0880s\n",
      "Epoch: 29, Loss: 0.00092905, Time: 0.0687s\n",
      "Epoch: 30, Loss: 0.00092153, Time: 0.0596s\n",
      "Epoch: 31, Loss: 0.00091413, Time: 0.0314s\n",
      "Epoch: 32, Loss: 0.00090684, Time: 0.0323s\n",
      "Epoch: 33, Loss: 0.00089967, Time: 0.0392s\n",
      "Epoch: 34, Loss: 0.00089261, Time: 0.0307s\n",
      "Epoch: 35, Loss: 0.00088565, Time: 0.0707s\n",
      "Epoch: 36, Loss: 0.00087880, Time: 0.0325s\n",
      "Epoch: 37, Loss: 0.00087206, Time: 0.0285s\n",
      "Epoch: 38, Loss: 0.00086541, Time: 0.0304s\n",
      "Epoch: 39, Loss: 0.00085886, Time: 0.0283s\n",
      "Epoch: 40, Loss: 0.00085241, Time: 0.0293s\n",
      "Epoch: 41, Loss: 0.00084606, Time: 0.0286s\n",
      "Epoch: 42, Loss: 0.00083979, Time: 0.0414s\n",
      "Epoch: 43, Loss: 0.00083362, Time: 0.0366s\n",
      "Epoch: 44, Loss: 0.00082753, Time: 0.0309s\n",
      "Epoch: 45, Loss: 0.00082153, Time: 0.0282s\n",
      "Epoch: 46, Loss: 0.00081561, Time: 0.0301s\n",
      "Epoch: 47, Loss: 0.00080978, Time: 0.0279s\n",
      "Epoch: 48, Loss: 0.00080403, Time: 0.0284s\n",
      "Epoch: 49, Loss: 0.00079835, Time: 0.0329s\n",
      "Epoch: 50, Loss: 0.00079276, Time: 0.0464s\n",
      "Epoch: 51, Loss: 0.00078724, Time: 0.0340s\n",
      "Epoch: 52, Loss: 0.00078179, Time: 0.0334s\n",
      "Epoch: 53, Loss: 0.00077642, Time: 0.0407s\n",
      "Epoch: 54, Loss: 0.00077112, Time: 0.0490s\n",
      "Epoch: 55, Loss: 0.00076589, Time: 0.0814s\n",
      "Epoch: 56, Loss: 0.00076073, Time: 0.0814s\n",
      "Epoch: 57, Loss: 0.00075564, Time: 0.0458s\n",
      "Epoch: 58, Loss: 0.00075061, Time: 0.0543s\n",
      "Epoch: 59, Loss: 0.00074565, Time: 0.0597s\n",
      "Epoch: 60, Loss: 0.00074075, Time: 0.0522s\n",
      "Epoch: 61, Loss: 0.00073592, Time: 0.0603s\n",
      "Epoch: 62, Loss: 0.00073114, Time: 0.0320s\n",
      "Epoch: 63, Loss: 0.00072643, Time: 0.0284s\n",
      "Epoch: 64, Loss: 0.00072177, Time: 0.0320s\n",
      "Epoch: 65, Loss: 0.00071718, Time: 0.0466s\n",
      "Epoch: 66, Loss: 0.00071264, Time: 0.0508s\n",
      "Epoch: 67, Loss: 0.00070815, Time: 0.0594s\n",
      "Epoch: 68, Loss: 0.00070372, Time: 0.0596s\n",
      "Epoch: 69, Loss: 0.00069935, Time: 0.0726s\n",
      "Epoch: 70, Loss: 0.00069502, Time: 0.0534s\n",
      "Epoch: 71, Loss: 0.00069075, Time: 0.0429s\n",
      "Epoch: 72, Loss: 0.00068653, Time: 0.0360s\n",
      "Epoch: 73, Loss: 0.00068236, Time: 0.0419s\n",
      "Epoch: 74, Loss: 0.00067824, Time: 0.0526s\n",
      "Epoch: 75, Loss: 0.00067417, Time: 0.0402s\n",
      "Epoch: 76, Loss: 0.00067014, Time: 0.0338s\n",
      "Epoch: 77, Loss: 0.00066617, Time: 0.0361s\n",
      "Epoch: 78, Loss: 0.00066223, Time: 0.0545s\n",
      "Epoch: 79, Loss: 0.00065835, Time: 0.0369s\n",
      "Epoch: 80, Loss: 0.00065450, Time: 0.0297s\n",
      "Epoch: 81, Loss: 0.00065070, Time: 0.0336s\n",
      "Epoch: 82, Loss: 0.00064695, Time: 0.0308s\n",
      "Epoch: 83, Loss: 0.00064323, Time: 0.0294s\n",
      "Epoch: 84, Loss: 0.00063956, Time: 0.0294s\n",
      "Epoch: 85, Loss: 0.00063593, Time: 0.0553s\n",
      "Epoch: 86, Loss: 0.00063234, Time: 0.0427s\n",
      "Epoch: 87, Loss: 0.00062878, Time: 0.0427s\n",
      "Epoch: 88, Loss: 0.00062527, Time: 0.0421s\n",
      "Epoch: 89, Loss: 0.00062179, Time: 0.0518s\n",
      "Epoch: 90, Loss: 0.00061836, Time: 0.0833s\n",
      "Epoch: 91, Loss: 0.00061495, Time: 0.0561s\n",
      "Epoch: 92, Loss: 0.00061159, Time: 0.0441s\n",
      "Epoch: 93, Loss: 0.00060826, Time: 0.0638s\n",
      "Epoch: 94, Loss: 0.00060497, Time: 0.0729s\n",
      "Epoch: 95, Loss: 0.00060171, Time: 0.1127s\n",
      "Epoch: 96, Loss: 0.00059848, Time: 0.1106s\n",
      "Epoch: 97, Loss: 0.00059529, Time: 0.0688s\n",
      "Epoch: 98, Loss: 0.00059213, Time: 0.0599s\n",
      "Epoch: 99, Loss: 0.00058901, Time: 0.0558s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# training\n",
    "for epoch in range(100):\n",
    "    err = []\n",
    "    upd = [0]*len(params)\n",
    "    \n",
    "    t0 = time.clock()\n",
    "    # for each data point, update our weights\n",
    "    for i in range(X.shape[0]):\n",
    "        loss,grad = train(X[i], T[i], *params)\n",
    "        #update loss\n",
    "        for j in range(len(params)):\n",
    "            params[j] -= upd[j]\n",
    "            \n",
    "        for j in range(len(params)):\n",
    "            upd[j] = learning_rate * grad[j] + momentum * upd[j]\n",
    "        \n",
    "        err.append(loss)\n",
    "    \n",
    "    print('Epoch: %d, Loss: %.8f, Time: %.4fs'%(epoch, np.mean(err), time.clock()-t0 ) )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR prediction\n",
      "[0 0 0 0 0 0 0 1 0 0]\n",
      "[1 1 1 1 1 1 1 0 1 1]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# try to predict something\n",
    "x = np.random.binomial(1, 0.5, n_in)\n",
    "\n",
    "print(\"XOR prediction\")\n",
    "print(x)\n",
    "print(predict(x, *params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "def double(x):\n",
    "    return 2*x\n",
    "\n",
    "def pipeOld(arg, *funcs):\n",
    "    for f in funcs:\n",
    "        arg = f(arg)\n",
    "    return arg\n",
    "\n",
    "print(pipe(2, double, str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def double(x):\n",
    "    return 2*x\n",
    "\n",
    "def pipe(arg, *funcs):\n",
    "    return reduce( (lambda a, b: b(a)), funcs, arg )\n",
    "\n",
    "print(pipe(2, double, double, str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
