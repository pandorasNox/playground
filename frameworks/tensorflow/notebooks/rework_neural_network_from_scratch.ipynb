{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Simplistic implementation of the two-layer neural network.\n",
    "Training method is stochastic (online) gradient descent with momentum.\n",
    "\n",
    "As an example it computes XOR for given input.\n",
    "\n",
    "Some details:\n",
    "- tanh activation for hidden layer\n",
    "- sigmoid activation for output layer\n",
    "- cross-entropy loss\n",
    "\n",
    "Less than 100 lines of active code.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "n_hidden = 10\n",
    "n_in = 10\n",
    "n_out = 10\n",
    "n_samples = 300\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "#np.random.seed(0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return  1 - np.tanh(x)**2\n",
    "\n",
    "def train(x, t, V, W, bv, bw):\n",
    "\n",
    "    # forward\n",
    "    A = np.dot(x, V) + bv\n",
    "    Z = np.tanh(A)\n",
    "\n",
    "    B = np.dot(Z, W) + bw\n",
    "    Y = sigmoid(B)\n",
    "\n",
    "    # backward\n",
    "    Ew = Y - t\n",
    "    Ev = tanh_prime(A) * np.dot(W, Ew)\n",
    "\n",
    "    dW = np.outer(Z, Ew)\n",
    "    dV = np.outer(x, Ev)\n",
    "\n",
    "    loss = -np.mean ( t * np.log(Y) + (1 - t) * np.log(1 - Y) )\n",
    "\n",
    "    # Note that we use error for each layer as a gradient\n",
    "    # for biases\n",
    "\n",
    "    return  loss, (dV, dW, Ev, Ew)\n",
    "\n",
    "def predict(x, V, W, bv, bw):\n",
    "    A = np.dot(x, V) + bv\n",
    "    B = np.dot(np.tanh(A), W) + bw\n",
    "    return (sigmoid(B) > 0.5).astype(int)\n",
    "\n",
    "# Setup initial parameters\n",
    "# Note that initialization is cruxial for first-order methods!\n",
    "\n",
    "V = np.random.normal(scale=0.1, size=(n_in, n_hidden))\n",
    "W = np.random.normal(scale=0.1, size=(n_hidden, n_out))\n",
    "\n",
    "bv = np.zeros(n_hidden)\n",
    "bw = np.zeros(n_out)\n",
    "\n",
    "params = [V,W,bv,bw]\n",
    "\n",
    "# Generate some data\n",
    "\n",
    "X = np.random.binomial(1, 0.5, (n_samples, n_in))\n",
    "T = X ^ 1\n",
    "\n",
    "# Train\n",
    "for epoch in range(100):\n",
    "    err = []\n",
    "    upd = [0]*len(params)\n",
    "\n",
    "    t0 = time.clock()\n",
    "    for i in range(X.shape[0]):\n",
    "        loss, grad = train(X[i], T[i], *params)\n",
    "\n",
    "        for j in range(len(params)):\n",
    "            params[j] -= upd[j]\n",
    "\n",
    "        for j in range(len(params)):\n",
    "            upd[j] = learning_rate * grad[j] + momentum * upd[j]\n",
    "\n",
    "        err.append( loss )\n",
    "\n",
    "    print (\"Epoch: %d, Loss: %.8f, Time: %.4fs\" % (\n",
    "                epoch, np.mean( err ), time.clock()-t0 ))\n",
    "\n",
    "# Try to predict something\n",
    "\n",
    "x = np.random.binomial(1, 0.5, n_in)\n",
    "print (\"XOR prediction:\")\n",
    "print (x)\n",
    "print (predict(x, *params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    {\"layerName\":\"input\", \"numberOfNodes\":10},\n",
    "    {\"layerName\":\"hidden0\", \"numberOfNodes\":10},\n",
    "    {\"layerName\":\"hidden1\", \"numberOfNodes\":10},\n",
    "    {\"layerName\":\"output\", \"numberOfNodes\":10},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numberOfSamples = 300\n",
    "inputSamples = np.random.binomial(1, 0.5, (numberOfSamples, layers[0][\"numberOfNodes\"]))\n",
    "targetSamples = inputSamples ^ 1 #create traget samples by simply invert the input samples\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ..., 0 1 0]\n",
      " [1 0 0 ..., 0 1 1]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " ..., \n",
      " [0 0 1 ..., 1 1 1]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " [0 1 1 ..., 1 1 1]]\n",
      "[[0 1 1 ..., 1 0 1]\n",
      " [0 1 1 ..., 1 0 0]\n",
      " [1 1 1 ..., 1 1 0]\n",
      " ..., \n",
      " [1 1 0 ..., 0 0 0]\n",
      " [0 1 1 ..., 1 1 1]\n",
      " [1 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# debug\n",
    "print(inputSamples)\n",
    "print(targetSamples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hidden layers\n",
    "for i in range(1, len(layers)-1): # only iterate over the mid layers, leave out first and last layer (input and output)\n",
    "    layers[i][\"weights\"] = np.random.normal(scale=0.1, size=(layers[i-1][\"numberOfNodes\"], layers[i][\"numberOfNodes\"]))\n",
    "    layers[i][\"bias\"] = np.zeros(layers[i+1][\"numberOfNodes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 1\n",
      "weights:\n",
      "[[-0.03146092 -0.07319642 -0.00839339 -0.11188275 -0.07475475 -0.12557313\n",
      "   0.12359797 -0.12568176  0.09978646  0.13069654]\n",
      " [ 0.16960256  0.00952491 -0.01594721 -0.01028723  0.21224227  0.00053433\n",
      "  -0.16479698  0.01146405  0.00662584  0.0674725 ]\n",
      " [-0.05950089 -0.07806738  0.03552074  0.01983741 -0.15436149  0.01321648\n",
      "  -0.03951212 -0.11676827  0.03413702 -0.08435535]\n",
      " [-0.05144317 -0.03858733  0.02636307 -0.00984572  0.12660463 -0.17338015\n",
      "   0.22112147  0.02182099  0.01350749 -0.03970833]\n",
      " [ 0.03441612 -0.09416352 -0.03612493 -0.05476971 -0.01149447  0.10712293\n",
      "  -0.08803129  0.0098169   0.19824704 -0.06478523]\n",
      " [ 0.12262981 -0.00425057 -0.25177428  0.01596778 -0.05479156 -0.10229381\n",
      "  -0.09902942 -0.00288189  0.04311854  0.11694531]\n",
      " [-0.05330592  0.09008884 -0.11226976 -0.15375181 -0.06201079  0.056049\n",
      "   0.10238065  0.03852106  0.18834893 -0.04070274]\n",
      " [ 0.10987134  0.11937343 -0.08569935 -0.05423213  0.03697317 -0.03186419\n",
      "  -0.10504311 -0.03306947  0.0736744   0.03633012]\n",
      " [ 0.05192565  0.00570634 -0.05520634  0.08293832 -0.02858953 -0.15649913\n",
      "  -0.24214343  0.01441718  0.05295336  0.24411197]\n",
      " [-0.03421122 -0.00143892 -0.01989612  0.00495903  0.03391215  0.03197297\n",
      "   0.01081955 -0.01698399 -0.06325626 -0.06605709]]\n",
      "bias:\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "\n",
      "hidden layer 2\n",
      "weights:\n",
      "[[ -5.27778168e-02   4.82615242e-02  -4.99803677e-02  -2.89748282e-02\n",
      "   -1.04056245e-01   1.20415509e-01  -4.64751274e-02  -2.49024873e-01\n",
      "    1.50392617e-01  -2.67228718e-01]\n",
      " [  1.17377900e-01   2.18187122e-02   3.37795182e-02   5.49902534e-02\n",
      "    1.05339479e-01  -9.81360843e-02  -3.58117112e-03   1.02228901e-01\n",
      "   -1.89660309e-01  -1.60276558e-01]\n",
      " [ -1.22894894e-01  -9.82470728e-02  -2.96275603e-03  -1.68737685e-01\n",
      "    2.77215496e-02  -2.81563150e-02  -1.07414057e-02  -1.36646191e-01\n",
      "    1.60699106e-01  -8.41868433e-02]\n",
      " [  6.34829267e-02  -8.60576985e-02  -7.07348019e-05  -1.17842241e-01\n",
      "    7.37032935e-02  -4.33173988e-02  -1.21849201e-04   1.05223891e-02\n",
      "    1.57572853e-01   1.13954375e-01]\n",
      " [ -1.65250206e-02  -7.44925941e-02  -2.69995298e-02  -1.10241024e-01\n",
      "   -1.24748177e-02  -6.97581550e-02  -9.41412570e-02  -2.82209719e-03\n",
      "    2.24910642e-02   2.51441010e-01]\n",
      " [ -1.18579362e-01  -3.36317212e-02   5.40960759e-02  -5.52106423e-02\n",
      "    6.66546884e-02  -5.52264942e-03   6.85323585e-03  -9.90974641e-02\n",
      "    1.72250808e-01  -5.99800914e-03]\n",
      " [  1.05808268e-01   7.74418465e-02   2.98214733e-02   4.26787449e-02\n",
      "    1.58088253e-01  -5.46492487e-03  -3.93273815e-02  -8.79445565e-03\n",
      "   -5.83991511e-02   6.89824763e-02]\n",
      " [ -1.62072569e-02  -2.41841685e-01  -1.03358557e-01   4.44146263e-02\n",
      "    1.15808898e-01  -1.81485286e-01   1.01210963e-01   9.67232074e-02\n",
      "    1.42403365e-01  -2.26546207e-01]\n",
      " [ -6.85116398e-02  -2.05915575e-01   4.92032818e-02   5.57279105e-02\n",
      "    8.57357915e-02   1.12507067e-01   3.09575230e-02   3.67458858e-02\n",
      "   -6.06089360e-02   2.57094293e-02]\n",
      " [ -1.72413779e-01  -1.23996211e-01   6.71572261e-02   8.68560243e-03\n",
      "    1.23191662e-01   9.17045638e-02   7.94031089e-03   4.36032891e-02\n",
      "   -4.10284261e-02   1.75617174e-02]]\n",
      "bias:\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# debug\n",
    "\n",
    "# print weights and bias of hidden layers\n",
    "for i in range(1, len(layers)-1):\n",
    "    print(\"hidden layer \" + str(i))\n",
    "    print(\"weights:\")\n",
    "    print(layers[i][\"weights\"])\n",
    "    print(\"bias:\")\n",
    "    print(layers[i][\"bias\"])\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define activation functions\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return  1 - np.tanh(x)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_loss_and_weight(inputSamples, targetSamples, listOfHiddenLayers):\n",
    "    return false\n",
    "    \n",
    "\n",
    "def train_old(x, t, V, W, bv, bw):\n",
    "\n",
    "    # forward propagation\n",
    "    A = np.dot(x, V) + bv\n",
    "    Z = np.tanh(A)\n",
    "\n",
    "    B = np.dot(Z, W) + bw\n",
    "    Y = sigmoid(B)\n",
    "\n",
    "    # backward propagation\n",
    "    Ew = Y - t\n",
    "    Ev = tanh_prime(A) * np.dot(W, Ew)\n",
    "\n",
    "    dW = np.outer(Z, Ew)\n",
    "    dV = np.outer(x, Ev)\n",
    "\n",
    "    loss = -np.mean ( t * np.log(Y) + (1 - t) * np.log(1 - Y) )\n",
    "\n",
    "    # Note that we use error for each layer as a gradient\n",
    "    # for biases\n",
    "\n",
    "    return  loss, (dV, dW, Ev, Ew)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old code\n",
    "\n",
    "\n",
    "def train(x, t, V, W, bv, bw):\n",
    "\n",
    "    # forward\n",
    "    A = np.dot(x, V) + bv\n",
    "    Z = np.tanh(A)\n",
    "\n",
    "    B = np.dot(Z, W) + bw\n",
    "    Y = sigmoid(B)\n",
    "\n",
    "    # backward\n",
    "    Ew = Y - t\n",
    "    Ev = tanh_prime(A) * np.dot(W, Ew)\n",
    "\n",
    "    dW = np.outer(Z, Ew)\n",
    "    dV = np.outer(x, Ev)\n",
    "\n",
    "    loss = -np.mean ( t * np.log(Y) + (1 - t) * np.log(1 - Y) )\n",
    "\n",
    "    # Note that we use error for each layer as a gradient\n",
    "    # for biases\n",
    "\n",
    "    return  loss, (dV, dW, Ev, Ew)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.43270164, Time: 0.0484s\n",
      "Epoch: 1, Loss: 0.13874419, Time: 0.0241s\n",
      "Epoch: 2, Loss: 0.09389856, Time: 0.0387s\n",
      "Epoch: 3, Loss: 0.07903321, Time: 0.0284s\n",
      "Epoch: 4, Loss: 0.07263637, Time: 0.0374s\n",
      "Epoch: 5, Loss: 0.06795481, Time: 0.0255s\n",
      "Epoch: 6, Loss: 0.06384804, Time: 0.0374s\n",
      "Epoch: 7, Loss: 0.06055031, Time: 0.0330s\n",
      "Epoch: 8, Loss: 0.06302405, Time: 0.0438s\n",
      "Epoch: 9, Loss: 0.07813340, Time: 0.0323s\n",
      "XOR prediction:\n",
      "[1 0 0 0 1 0 0 1 0 0]\n",
      "[0 1 1 1 0 1 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def predict(x, V, W, bv, bw):\n",
    "    A = np.dot(x, V) + bv\n",
    "    B = np.dot(np.tanh(A), W) + bw\n",
    "    return (sigmoid(B) > 0.5).astype(int)\n",
    "\n",
    "params = [\n",
    "    layers[1][\"weights\"],\n",
    "    layers[2][\"weights\"],\n",
    "    layers[1][\"bias\"],\n",
    "    layers[2][\"bias\"],\n",
    "]\n",
    "\n",
    "X = inputSamples\n",
    "T = targetSamples\n",
    "\n",
    "# Train\n",
    "for epoch in range(10):\n",
    "    err = []\n",
    "    upd = [0]*len(params)\n",
    "\n",
    "    t0 = time.clock()\n",
    "    for i in range(X.shape[0]):\n",
    "        loss, grad = train(X[i], T[i], *params)\n",
    "        \n",
    "        # is there a mathematical reason for that???\n",
    "        #for j in range(len(params)):\n",
    "        #    params[j] -= upd[j]\n",
    "\n",
    "        for j in range(len(params)):\n",
    "            upd[j] = learning_rate * grad[j] + momentum * upd[j]\n",
    "            \n",
    "        for j in range(len(params)-2):\n",
    "            params[j] -= upd[j]\n",
    "\n",
    "        err.append( loss )\n",
    "\n",
    "    print (\"Epoch: %d, Loss: %.8f, Time: %.4fs\" % (\n",
    "                epoch, np.mean( err ), time.clock()-t0 ))\n",
    "\n",
    "# Try to predict something\n",
    "\n",
    "x = np.random.binomial(1, 0.5, layers[0][\"numberOfNodes\"])\n",
    "print (\"XOR prediction:\")\n",
    "print (x)\n",
    "print (predict(x, *params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
