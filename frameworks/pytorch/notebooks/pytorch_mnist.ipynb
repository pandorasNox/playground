{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "0.4.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "print(torch.__version__)\n",
    "print(\"0.4.0\" == torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kwargs = {}\n",
    "\n",
    "train = dataloader(\n",
    "    MNIST(\n",
    "        'data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    ),\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: ./data\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "\n",
      "test_data: Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Split: test\n",
      "    Root Location: ./data\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                         )\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "train_data = MNIST('./data', train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "test_data = MNIST('./data', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "print(\"train_data:\", train_data)\n",
    "print(\"\")\n",
    "print(\"test_data:\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data [[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "dataloader_args = dict(shuffle=True, batch_size=64, num_workers=1, pin_memory=True)\n",
    "train_loader = dataloader.DataLoader(train_data, **dataloader_args)\n",
    "test_loader = dataloader.DataLoader(test_data, **dataloader_args)\n",
    "\n",
    "print(\"train_data\", train_data.train_data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]\n",
      " - Numpy Shape: (60000, 28, 28)\n",
      " - Tensor Shape: torch.Size([60000, 28, 28])\n",
      " - Transformed Shape: torch.Size([28, 60000, 28])\n",
      " - min: tensor(0.)\n",
      " - max: tensor(1.)\n",
      " - mean: tensor(0.1307)\n",
      " - std: tensor(0.3081)\n",
      " - var: tensor(1.00000e-02 *\n",
      "       9.4930)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train_data = train_data.train_data\n",
    "transformed_train_data = train_data.transform(train_data.train_data.cpu().numpy())\n",
    "\n",
    "print('[Train]')\n",
    "print(' - Numpy Shape:', train_data.train_data.cpu().numpy().shape)\n",
    "print(' - Tensor Shape:', train_data.train_data.size())\n",
    "print(' - Transformed Shape:', transformed_train_data.size())\n",
    "print(' - min:', torch.min(transformed_train_data))\n",
    "print(' - max:', torch.max(transformed_train_data))\n",
    "print(' - mean:', torch.mean(transformed_train_data))\n",
    "print(' - std:', torch.std(transformed_train_data))\n",
    "print(' - var:', torch.var(transformed_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createOptimizer(net):\n",
    "    return optim.SGD(net.parameters(), lr=0.001, momentum=0.8)\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_epoch_train(model, current_epoch, train_data_loader, optimizer):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    #model.eval()\n",
    "\n",
    "    for batch_id, (data, target) in enumerate(train_data_loader):\n",
    "        #if torch.cuda.is_available():\n",
    "        #if cuda #<= if gpu support, do something like\n",
    "            #data = data.cuda()\n",
    "            #target = target.cuda()\n",
    "            \n",
    "        #transform to variables\n",
    "        data = Variable(data)\n",
    "        target = Variable(target)\n",
    "        \n",
    "        #optimizer = createOptimizer(model) #<= not needed bec is passed\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(data) #out OR prediction\n",
    "        #criterion = nn.CrossEntropyLoss\n",
    "        criterion = F.nll_loss # criterion-fn\n",
    "        loss = criterion(out, target)\n",
    "        \n",
    "        batch_losses.append(loss.data[0])\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        print('\\r Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                current_epoch, \n",
    "                batch_id * len(data), \n",
    "                len(train_data_loader.dataset),\n",
    "                100. * batch_id / len(train_data_loader), \n",
    "                loss.data[0]\n",
    "            ),\n",
    "            end=''\n",
    "        )\n",
    "        \n",
    "    return (model, batch_losses)\n",
    "\n",
    "def train(net, train_data_loader, optimizer, count_epochs=15):\n",
    "    #check count_epochs >= 1\n",
    "    losses = []\n",
    "\n",
    "    for current_epoch in range(1, count_epochs):\n",
    "        (returnedNet, batch_losses) = single_epoch_train(net, current_epoch, train_data_loader, optimizer)\n",
    "        net = returnedNet\n",
    "        losses += batch_losses\n",
    "\n",
    "    return (net, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        \n",
    "        #add layers\n",
    "        #input_features = 10\n",
    "        conv1_inChannels = 1\n",
    "        conv1_outChannels = 10\n",
    "        conv1_kernalSize = 5\n",
    "        self.conv1 = nn.Conv2d(conv1_inChannels, conv1_outChannels, conv1_kernalSize)\n",
    "        \n",
    "        conv2_inChannels = conv1_outChannels\n",
    "        conv2_outChannels = 20\n",
    "        conv2_kernalSize = 5\n",
    "        self.conv2 = nn.Conv2d(conv2_inChannels, conv2_outChannels, conv2_kernalSize)\n",
    "        \n",
    "        self.conv_dropout = nn.Dropout2d\n",
    "        \n",
    "        self.fc1 = nn.Linear(320, 60) #fc1 = fully connected layer #320 = amount of pixels per image\n",
    "        self.fc2 = nn.Linear(60, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #conv1\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #conv2        \n",
    "        x = self.conv2(x)\n",
    "        #x = self.conv_dropout(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #step to determine fc1 inputs??? = result leads to 320, but why, how to calc that???\n",
    "        #print(x.size())\n",
    "        #exit()\n",
    "        \n",
    "        #why???\n",
    "        x = x.view(-1, 320)\n",
    "        \n",
    "        #fc1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #fc2 ~ output\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # helper function ??? why ???\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # attention: mini batch / stochachistig grad\n",
    "        num = 1\n",
    "        for i in size:\n",
    "            num *= i\n",
    "        return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 4 [29984/60000 (100%)]\tLoss: 0.116158"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MNISTNet(\n",
       "   (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "   (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "   (fc1): Linear(in_features=320, out_features=60, bias=True)\n",
       "   (fc2): Linear(in_features=60, out_features=10, bias=True)\n",
       " ),\n",
       " [tensor(2.3202),\n",
       "  tensor(2.2877),\n",
       "  tensor(2.3063),\n",
       "  tensor(2.3011),\n",
       "  tensor(2.2990),\n",
       "  tensor(2.3046),\n",
       "  tensor(2.3190),\n",
       "  tensor(2.3141),\n",
       "  tensor(2.3026),\n",
       "  tensor(2.3049),\n",
       "  tensor(2.3107),\n",
       "  tensor(2.3106),\n",
       "  tensor(2.2917),\n",
       "  tensor(2.3210),\n",
       "  tensor(2.3120),\n",
       "  tensor(2.2972),\n",
       "  tensor(2.3063),\n",
       "  tensor(2.3085),\n",
       "  tensor(2.3057),\n",
       "  tensor(2.3047),\n",
       "  tensor(2.3037),\n",
       "  tensor(2.3064),\n",
       "  tensor(2.2997),\n",
       "  tensor(2.3090),\n",
       "  tensor(2.3128),\n",
       "  tensor(2.2976),\n",
       "  tensor(2.3139),\n",
       "  tensor(2.2891),\n",
       "  tensor(2.2804),\n",
       "  tensor(2.2961),\n",
       "  tensor(2.3036),\n",
       "  tensor(2.3037),\n",
       "  tensor(2.3105),\n",
       "  tensor(2.3024),\n",
       "  tensor(2.3020),\n",
       "  tensor(2.3070),\n",
       "  tensor(2.3018),\n",
       "  tensor(2.3049),\n",
       "  tensor(2.2979),\n",
       "  tensor(2.2970),\n",
       "  tensor(2.2957),\n",
       "  tensor(2.2975),\n",
       "  tensor(2.2935),\n",
       "  tensor(2.2956),\n",
       "  tensor(2.2976),\n",
       "  tensor(2.3054),\n",
       "  tensor(2.3011),\n",
       "  tensor(2.2976),\n",
       "  tensor(2.2980),\n",
       "  tensor(2.2996),\n",
       "  tensor(2.2977),\n",
       "  tensor(2.2953),\n",
       "  tensor(2.3039),\n",
       "  tensor(2.2883),\n",
       "  tensor(2.2963),\n",
       "  tensor(2.2935),\n",
       "  tensor(2.2796),\n",
       "  tensor(2.2892),\n",
       "  tensor(2.3034),\n",
       "  tensor(2.3064),\n",
       "  tensor(2.2917),\n",
       "  tensor(2.3028),\n",
       "  tensor(2.3046),\n",
       "  tensor(2.2920),\n",
       "  tensor(2.2973),\n",
       "  tensor(2.2985),\n",
       "  tensor(2.3040),\n",
       "  tensor(2.2998),\n",
       "  tensor(2.3017),\n",
       "  tensor(2.3012),\n",
       "  tensor(2.3043),\n",
       "  tensor(2.2998),\n",
       "  tensor(2.2952),\n",
       "  tensor(2.2992),\n",
       "  tensor(2.2974),\n",
       "  tensor(2.2927),\n",
       "  tensor(2.3001),\n",
       "  tensor(2.3003),\n",
       "  tensor(2.3027),\n",
       "  tensor(2.2991),\n",
       "  tensor(2.3129),\n",
       "  tensor(2.2886),\n",
       "  tensor(2.2974),\n",
       "  tensor(2.2917),\n",
       "  tensor(2.2900),\n",
       "  tensor(2.3032),\n",
       "  tensor(2.2999),\n",
       "  tensor(2.2884),\n",
       "  tensor(2.3018),\n",
       "  tensor(2.3040),\n",
       "  tensor(2.2967),\n",
       "  tensor(2.2926),\n",
       "  tensor(2.3026),\n",
       "  tensor(2.2915),\n",
       "  tensor(2.3024),\n",
       "  tensor(2.2921),\n",
       "  tensor(2.3161),\n",
       "  tensor(2.2859),\n",
       "  tensor(2.3000),\n",
       "  tensor(2.2950),\n",
       "  tensor(2.2952),\n",
       "  tensor(2.2917),\n",
       "  tensor(2.2915),\n",
       "  tensor(2.2901),\n",
       "  tensor(2.2910),\n",
       "  tensor(2.2911),\n",
       "  tensor(2.2894),\n",
       "  tensor(2.2932),\n",
       "  tensor(2.2945),\n",
       "  tensor(2.2859),\n",
       "  tensor(2.2953),\n",
       "  tensor(2.2890),\n",
       "  tensor(2.3015),\n",
       "  tensor(2.2935),\n",
       "  tensor(2.2958),\n",
       "  tensor(2.2875),\n",
       "  tensor(2.3000),\n",
       "  tensor(2.2929),\n",
       "  tensor(2.2936),\n",
       "  tensor(2.2983),\n",
       "  tensor(2.2879),\n",
       "  tensor(2.2990),\n",
       "  tensor(2.2926),\n",
       "  tensor(2.2823),\n",
       "  tensor(2.2868),\n",
       "  tensor(2.2996),\n",
       "  tensor(2.2949),\n",
       "  tensor(2.2845),\n",
       "  tensor(2.3022),\n",
       "  tensor(2.2884),\n",
       "  tensor(2.2797),\n",
       "  tensor(2.2897),\n",
       "  tensor(2.2904),\n",
       "  tensor(2.2890),\n",
       "  tensor(2.2981),\n",
       "  tensor(2.2863),\n",
       "  tensor(2.2963),\n",
       "  tensor(2.2874),\n",
       "  tensor(2.2826),\n",
       "  tensor(2.2894),\n",
       "  tensor(2.2865),\n",
       "  tensor(2.2953),\n",
       "  tensor(2.2839),\n",
       "  tensor(2.2817),\n",
       "  tensor(2.2851),\n",
       "  tensor(2.2779),\n",
       "  tensor(2.2808),\n",
       "  tensor(2.2907),\n",
       "  tensor(2.2923),\n",
       "  tensor(2.2852),\n",
       "  tensor(2.2815),\n",
       "  tensor(2.2874),\n",
       "  tensor(2.2909),\n",
       "  tensor(2.2872),\n",
       "  tensor(2.2877),\n",
       "  tensor(2.2826),\n",
       "  tensor(2.2800),\n",
       "  tensor(2.2858),\n",
       "  tensor(2.2973),\n",
       "  tensor(2.2972),\n",
       "  tensor(2.2857),\n",
       "  tensor(2.2816),\n",
       "  tensor(2.2795),\n",
       "  tensor(2.2770),\n",
       "  tensor(2.2877),\n",
       "  tensor(2.2885),\n",
       "  tensor(2.2945),\n",
       "  tensor(2.2711),\n",
       "  tensor(2.2914),\n",
       "  tensor(2.2813),\n",
       "  tensor(2.2849),\n",
       "  tensor(2.2771),\n",
       "  tensor(2.2837),\n",
       "  tensor(2.2905),\n",
       "  tensor(2.2799),\n",
       "  tensor(2.2808),\n",
       "  tensor(2.2892),\n",
       "  tensor(2.2837),\n",
       "  tensor(2.2759),\n",
       "  tensor(2.2857),\n",
       "  tensor(2.2845),\n",
       "  tensor(2.2608),\n",
       "  tensor(2.2895),\n",
       "  tensor(2.2837),\n",
       "  tensor(2.2783),\n",
       "  tensor(2.3014),\n",
       "  tensor(2.2823),\n",
       "  tensor(2.2766),\n",
       "  tensor(2.2855),\n",
       "  tensor(2.2894),\n",
       "  tensor(2.2828),\n",
       "  tensor(2.2871),\n",
       "  tensor(2.2830),\n",
       "  tensor(2.2864),\n",
       "  tensor(2.2780),\n",
       "  tensor(2.2762),\n",
       "  tensor(2.2797),\n",
       "  tensor(2.2734),\n",
       "  tensor(2.2856),\n",
       "  tensor(2.2868),\n",
       "  tensor(2.2846),\n",
       "  tensor(2.2852),\n",
       "  tensor(2.2776),\n",
       "  tensor(2.2927),\n",
       "  tensor(2.2859),\n",
       "  tensor(2.2816),\n",
       "  tensor(2.2767),\n",
       "  tensor(2.2920),\n",
       "  tensor(2.2773),\n",
       "  tensor(2.2739),\n",
       "  tensor(2.2822),\n",
       "  tensor(2.2708),\n",
       "  tensor(2.2826),\n",
       "  tensor(2.2788),\n",
       "  tensor(2.2699),\n",
       "  tensor(2.2694),\n",
       "  tensor(2.2786),\n",
       "  tensor(2.2877),\n",
       "  tensor(2.2844),\n",
       "  tensor(2.2724),\n",
       "  tensor(2.2719),\n",
       "  tensor(2.2662),\n",
       "  tensor(2.2634),\n",
       "  tensor(2.2925),\n",
       "  tensor(2.2784),\n",
       "  tensor(2.2726),\n",
       "  tensor(2.2708),\n",
       "  tensor(2.2803),\n",
       "  tensor(2.2658),\n",
       "  tensor(2.2707),\n",
       "  tensor(2.2671),\n",
       "  tensor(2.2782),\n",
       "  tensor(2.2828),\n",
       "  tensor(2.2709),\n",
       "  tensor(2.2768),\n",
       "  tensor(2.2702),\n",
       "  tensor(2.2712),\n",
       "  tensor(2.2680),\n",
       "  tensor(2.2769),\n",
       "  tensor(2.2754),\n",
       "  tensor(2.2805),\n",
       "  tensor(2.2764),\n",
       "  tensor(2.2722),\n",
       "  tensor(2.2599),\n",
       "  tensor(2.2723),\n",
       "  tensor(2.2695),\n",
       "  tensor(2.2725),\n",
       "  tensor(2.2748),\n",
       "  tensor(2.2827),\n",
       "  tensor(2.2770),\n",
       "  tensor(2.2780),\n",
       "  tensor(2.2707),\n",
       "  tensor(2.2745),\n",
       "  tensor(2.2691),\n",
       "  tensor(2.2673),\n",
       "  tensor(2.2611),\n",
       "  tensor(2.2640),\n",
       "  tensor(2.2604),\n",
       "  tensor(2.2615),\n",
       "  tensor(2.2616),\n",
       "  tensor(2.2645),\n",
       "  tensor(2.2660),\n",
       "  tensor(2.2765),\n",
       "  tensor(2.2814),\n",
       "  tensor(2.2716),\n",
       "  tensor(2.2668),\n",
       "  tensor(2.2743),\n",
       "  tensor(2.2771),\n",
       "  tensor(2.2664),\n",
       "  tensor(2.2673),\n",
       "  tensor(2.2762),\n",
       "  tensor(2.2661),\n",
       "  tensor(2.2660),\n",
       "  tensor(2.2819),\n",
       "  tensor(2.2603),\n",
       "  tensor(2.2674),\n",
       "  tensor(2.2632),\n",
       "  tensor(2.2633),\n",
       "  tensor(2.2626),\n",
       "  tensor(2.2608),\n",
       "  tensor(2.2665),\n",
       "  tensor(2.2590),\n",
       "  tensor(2.2690),\n",
       "  tensor(2.2647),\n",
       "  tensor(2.2605),\n",
       "  tensor(2.2576),\n",
       "  tensor(2.2552),\n",
       "  tensor(2.2502),\n",
       "  tensor(2.2728),\n",
       "  tensor(2.2593),\n",
       "  tensor(2.2685),\n",
       "  tensor(2.2690),\n",
       "  tensor(2.2722),\n",
       "  tensor(2.2473),\n",
       "  tensor(2.2772),\n",
       "  tensor(2.2574),\n",
       "  tensor(2.2665),\n",
       "  tensor(2.2602),\n",
       "  tensor(2.2553),\n",
       "  tensor(2.2641),\n",
       "  tensor(2.2464),\n",
       "  tensor(2.2659),\n",
       "  tensor(2.2605),\n",
       "  tensor(2.2684),\n",
       "  tensor(2.2682),\n",
       "  tensor(2.2585),\n",
       "  tensor(2.2655),\n",
       "  tensor(2.2597),\n",
       "  tensor(2.2568),\n",
       "  tensor(2.2554),\n",
       "  tensor(2.2654),\n",
       "  tensor(2.2533),\n",
       "  tensor(2.2517),\n",
       "  tensor(2.2477),\n",
       "  tensor(2.2522),\n",
       "  tensor(2.2604),\n",
       "  tensor(2.2569),\n",
       "  tensor(2.2605),\n",
       "  tensor(2.2550),\n",
       "  tensor(2.2656),\n",
       "  tensor(2.2401),\n",
       "  tensor(2.2495),\n",
       "  tensor(2.2504),\n",
       "  tensor(2.2498),\n",
       "  tensor(2.2534),\n",
       "  tensor(2.2560),\n",
       "  tensor(2.2468),\n",
       "  tensor(2.2656),\n",
       "  tensor(2.2576),\n",
       "  tensor(2.2427),\n",
       "  tensor(2.2535),\n",
       "  tensor(2.2584),\n",
       "  tensor(2.2530),\n",
       "  tensor(2.2494),\n",
       "  tensor(2.2502),\n",
       "  tensor(2.2492),\n",
       "  tensor(2.2630),\n",
       "  tensor(2.2401),\n",
       "  tensor(2.2452),\n",
       "  tensor(2.2454),\n",
       "  tensor(2.2378),\n",
       "  tensor(2.2467),\n",
       "  tensor(2.2497),\n",
       "  tensor(2.2393),\n",
       "  tensor(2.2592),\n",
       "  tensor(2.2473),\n",
       "  tensor(2.2646),\n",
       "  tensor(2.2394),\n",
       "  tensor(2.2452),\n",
       "  tensor(2.2467),\n",
       "  tensor(2.2341),\n",
       "  tensor(2.2378),\n",
       "  tensor(2.2458),\n",
       "  tensor(2.2457),\n",
       "  tensor(2.2415),\n",
       "  tensor(2.2449),\n",
       "  tensor(2.2428),\n",
       "  tensor(2.2606),\n",
       "  tensor(2.2426),\n",
       "  tensor(2.2510),\n",
       "  tensor(2.2520),\n",
       "  tensor(2.2397),\n",
       "  tensor(2.2512),\n",
       "  tensor(2.2480),\n",
       "  tensor(2.2467),\n",
       "  tensor(2.2418),\n",
       "  tensor(2.2337),\n",
       "  tensor(2.2349),\n",
       "  tensor(2.2391),\n",
       "  tensor(2.2335),\n",
       "  tensor(2.2508),\n",
       "  tensor(2.2241),\n",
       "  tensor(2.2334),\n",
       "  tensor(2.2436),\n",
       "  tensor(2.2356),\n",
       "  tensor(2.2327),\n",
       "  tensor(2.2406),\n",
       "  tensor(2.2338),\n",
       "  tensor(2.2417),\n",
       "  tensor(2.2330),\n",
       "  tensor(2.2493),\n",
       "  tensor(2.2374),\n",
       "  tensor(2.2380),\n",
       "  tensor(2.2269),\n",
       "  tensor(2.2231),\n",
       "  tensor(2.2314),\n",
       "  tensor(2.2336),\n",
       "  tensor(2.2421),\n",
       "  tensor(2.2360),\n",
       "  tensor(2.2338),\n",
       "  tensor(2.2324),\n",
       "  tensor(2.2293),\n",
       "  tensor(2.2328),\n",
       "  tensor(2.2411),\n",
       "  tensor(2.2374),\n",
       "  tensor(2.2325),\n",
       "  tensor(2.2314),\n",
       "  tensor(2.2428),\n",
       "  tensor(2.2281),\n",
       "  tensor(2.2239),\n",
       "  tensor(2.2311),\n",
       "  tensor(2.2431),\n",
       "  tensor(2.2189),\n",
       "  tensor(2.2214),\n",
       "  tensor(2.2273),\n",
       "  tensor(2.2122),\n",
       "  tensor(2.2425),\n",
       "  tensor(2.2231),\n",
       "  tensor(2.2327),\n",
       "  tensor(2.2427),\n",
       "  tensor(2.2140),\n",
       "  tensor(2.2224),\n",
       "  tensor(2.2267),\n",
       "  tensor(2.2265),\n",
       "  tensor(2.2260),\n",
       "  tensor(2.2225),\n",
       "  tensor(2.2274),\n",
       "  tensor(2.2408),\n",
       "  tensor(2.2035),\n",
       "  tensor(2.2246),\n",
       "  tensor(2.2375),\n",
       "  tensor(2.2248),\n",
       "  tensor(2.2199),\n",
       "  tensor(2.2238),\n",
       "  tensor(2.2177),\n",
       "  tensor(2.2330),\n",
       "  tensor(2.2135),\n",
       "  tensor(2.2104),\n",
       "  tensor(2.2082),\n",
       "  tensor(2.2210),\n",
       "  tensor(2.2066),\n",
       "  tensor(2.2157),\n",
       "  tensor(2.2155),\n",
       "  tensor(2.2168),\n",
       "  tensor(2.2200),\n",
       "  tensor(2.2081),\n",
       "  tensor(2.2126),\n",
       "  tensor(2.2180),\n",
       "  tensor(2.2011),\n",
       "  tensor(2.2196),\n",
       "  tensor(2.2101),\n",
       "  tensor(2.2249),\n",
       "  tensor(2.2090),\n",
       "  tensor(2.1975),\n",
       "  tensor(2.2128),\n",
       "  tensor(2.2013),\n",
       "  tensor(2.2115),\n",
       "  tensor(2.2126),\n",
       "  tensor(2.2054),\n",
       "  tensor(2.2045),\n",
       "  tensor(2.2003),\n",
       "  tensor(2.1926),\n",
       "  tensor(2.1962),\n",
       "  tensor(2.1920),\n",
       "  tensor(2.2027),\n",
       "  tensor(2.2049),\n",
       "  tensor(2.1933),\n",
       "  tensor(2.2132),\n",
       "  tensor(2.1942),\n",
       "  tensor(2.1943),\n",
       "  tensor(2.2053),\n",
       "  tensor(2.1878),\n",
       "  tensor(2.1906),\n",
       "  tensor(2.2064),\n",
       "  tensor(2.2232),\n",
       "  tensor(2.1988),\n",
       "  tensor(2.2346),\n",
       "  tensor(2.1961),\n",
       "  tensor(2.1851),\n",
       "  tensor(2.1910),\n",
       "  tensor(2.1883),\n",
       "  tensor(2.2122),\n",
       "  tensor(2.1939),\n",
       "  tensor(2.1866),\n",
       "  tensor(2.2076),\n",
       "  tensor(2.1781),\n",
       "  tensor(2.1784),\n",
       "  tensor(2.1830),\n",
       "  tensor(2.2052),\n",
       "  tensor(2.1831),\n",
       "  tensor(2.1880),\n",
       "  tensor(2.2095),\n",
       "  tensor(2.1831),\n",
       "  tensor(2.1723),\n",
       "  tensor(2.1827),\n",
       "  tensor(2.1901),\n",
       "  tensor(2.1924),\n",
       "  tensor(2.1760),\n",
       "  tensor(2.1972),\n",
       "  tensor(2.1984),\n",
       "  tensor(2.1765),\n",
       "  tensor(2.2139),\n",
       "  tensor(2.1686),\n",
       "  tensor(2.1773),\n",
       "  tensor(2.1864),\n",
       "  tensor(2.1725),\n",
       "  tensor(2.1885),\n",
       "  tensor(2.1907),\n",
       "  tensor(2.1701),\n",
       "  tensor(2.1869),\n",
       "  tensor(2.1779),\n",
       "  tensor(2.1639),\n",
       "  tensor(2.1525),\n",
       "  tensor(2.1857),\n",
       "  tensor(2.1579),\n",
       "  tensor(2.1614),\n",
       "  tensor(2.1659),\n",
       "  tensor(2.1724),\n",
       "  tensor(2.1560),\n",
       "  tensor(2.1705),\n",
       "  tensor(2.1663),\n",
       "  tensor(2.1757),\n",
       "  tensor(2.1849),\n",
       "  tensor(2.1672),\n",
       "  tensor(2.1797),\n",
       "  tensor(2.1565),\n",
       "  tensor(2.1781),\n",
       "  tensor(2.1530),\n",
       "  tensor(2.1575),\n",
       "  tensor(2.1603),\n",
       "  tensor(2.1690),\n",
       "  tensor(2.1446),\n",
       "  tensor(2.1672),\n",
       "  tensor(2.1401),\n",
       "  tensor(2.1790),\n",
       "  tensor(2.1578),\n",
       "  tensor(2.1353),\n",
       "  tensor(2.1273),\n",
       "  tensor(2.1544),\n",
       "  tensor(2.1527),\n",
       "  tensor(2.1522),\n",
       "  tensor(2.1547),\n",
       "  tensor(2.1472),\n",
       "  tensor(2.1546),\n",
       "  tensor(2.1499),\n",
       "  tensor(2.1409),\n",
       "  tensor(2.1449),\n",
       "  tensor(2.1574),\n",
       "  tensor(2.1555),\n",
       "  tensor(2.1474),\n",
       "  tensor(2.1322),\n",
       "  tensor(2.1726),\n",
       "  tensor(2.1581),\n",
       "  tensor(2.1423),\n",
       "  tensor(2.1526),\n",
       "  tensor(2.1435),\n",
       "  tensor(2.1224),\n",
       "  tensor(2.1337),\n",
       "  tensor(2.1316),\n",
       "  tensor(2.1295),\n",
       "  tensor(2.1495),\n",
       "  tensor(2.1257),\n",
       "  tensor(2.1117),\n",
       "  tensor(2.1424),\n",
       "  tensor(2.1379),\n",
       "  tensor(2.1125),\n",
       "  tensor(2.1324),\n",
       "  tensor(2.1059),\n",
       "  tensor(2.1251),\n",
       "  tensor(2.1447),\n",
       "  tensor(2.1442),\n",
       "  tensor(2.1336),\n",
       "  tensor(2.1104),\n",
       "  tensor(2.0852),\n",
       "  tensor(2.1106),\n",
       "  tensor(2.1155),\n",
       "  tensor(2.1082),\n",
       "  tensor(2.1013),\n",
       "  tensor(2.1007),\n",
       "  tensor(2.0971),\n",
       "  tensor(2.1147),\n",
       "  tensor(2.0915),\n",
       "  tensor(2.1127),\n",
       "  tensor(2.1064),\n",
       "  tensor(2.1203),\n",
       "  tensor(2.1008),\n",
       "  tensor(2.0806),\n",
       "  tensor(2.1133),\n",
       "  tensor(2.0594),\n",
       "  tensor(2.1065),\n",
       "  tensor(2.1158),\n",
       "  tensor(2.0906),\n",
       "  tensor(2.1133),\n",
       "  tensor(2.0692),\n",
       "  tensor(2.0517),\n",
       "  tensor(2.0791),\n",
       "  tensor(2.0573),\n",
       "  tensor(2.0862),\n",
       "  tensor(2.0629),\n",
       "  tensor(2.0723),\n",
       "  tensor(2.0839),\n",
       "  tensor(2.0413),\n",
       "  tensor(2.1065),\n",
       "  tensor(2.0663),\n",
       "  tensor(2.0858),\n",
       "  tensor(2.0643),\n",
       "  tensor(2.1005),\n",
       "  tensor(2.1023),\n",
       "  tensor(2.0570),\n",
       "  tensor(2.0809),\n",
       "  tensor(2.0734),\n",
       "  tensor(2.0720),\n",
       "  tensor(2.0615),\n",
       "  tensor(2.0286),\n",
       "  tensor(2.0841),\n",
       "  tensor(2.0643),\n",
       "  tensor(2.0058),\n",
       "  tensor(2.0531),\n",
       "  tensor(2.0139),\n",
       "  tensor(2.0259),\n",
       "  tensor(2.0467),\n",
       "  tensor(2.0309),\n",
       "  tensor(2.0448),\n",
       "  tensor(2.0293),\n",
       "  tensor(2.0654),\n",
       "  tensor(2.0117),\n",
       "  tensor(2.0477),\n",
       "  tensor(2.0322),\n",
       "  tensor(2.0094),\n",
       "  tensor(2.0274),\n",
       "  tensor(2.0264),\n",
       "  tensor(1.9933),\n",
       "  tensor(2.0329),\n",
       "  tensor(2.0717),\n",
       "  tensor(2.0613),\n",
       "  tensor(2.0145),\n",
       "  tensor(2.0625),\n",
       "  tensor(1.9949),\n",
       "  tensor(2.0196),\n",
       "  tensor(1.9490),\n",
       "  tensor(1.9997),\n",
       "  tensor(2.0227),\n",
       "  tensor(1.9827),\n",
       "  tensor(1.9918),\n",
       "  tensor(2.0427),\n",
       "  tensor(1.9884),\n",
       "  tensor(1.9962),\n",
       "  tensor(2.0223),\n",
       "  tensor(2.0372),\n",
       "  tensor(1.9877),\n",
       "  tensor(2.0422),\n",
       "  tensor(2.0259),\n",
       "  tensor(1.9587),\n",
       "  tensor(1.9803),\n",
       "  tensor(2.0228),\n",
       "  tensor(1.9486),\n",
       "  tensor(1.9577),\n",
       "  tensor(1.9638),\n",
       "  tensor(1.9402),\n",
       "  tensor(1.9822),\n",
       "  tensor(1.9068),\n",
       "  tensor(1.9298),\n",
       "  tensor(1.9808),\n",
       "  tensor(2.0252),\n",
       "  tensor(1.9823),\n",
       "  tensor(2.0161),\n",
       "  tensor(1.9175),\n",
       "  tensor(1.9111),\n",
       "  tensor(1.9012),\n",
       "  tensor(1.9377),\n",
       "  tensor(2.0109),\n",
       "  tensor(2.0052),\n",
       "  tensor(1.9468),\n",
       "  tensor(1.9515),\n",
       "  tensor(1.8785),\n",
       "  tensor(1.9115),\n",
       "  tensor(1.9315),\n",
       "  tensor(1.9193),\n",
       "  tensor(1.8689),\n",
       "  tensor(1.9473),\n",
       "  tensor(1.8874),\n",
       "  tensor(1.8885),\n",
       "  tensor(1.8923),\n",
       "  tensor(1.9337),\n",
       "  tensor(2.0023),\n",
       "  tensor(1.9116),\n",
       "  tensor(1.8867),\n",
       "  tensor(1.8649),\n",
       "  tensor(1.9364),\n",
       "  tensor(1.9101),\n",
       "  tensor(1.9084),\n",
       "  tensor(1.8953),\n",
       "  tensor(1.7959),\n",
       "  tensor(1.8230),\n",
       "  tensor(1.9086),\n",
       "  tensor(1.8662),\n",
       "  tensor(1.8995),\n",
       "  tensor(1.8998),\n",
       "  tensor(1.9323),\n",
       "  tensor(1.8114),\n",
       "  tensor(1.8133),\n",
       "  tensor(1.8269),\n",
       "  tensor(1.8132),\n",
       "  tensor(1.8544),\n",
       "  tensor(1.8787),\n",
       "  tensor(1.8850),\n",
       "  tensor(1.8496),\n",
       "  tensor(1.8765),\n",
       "  tensor(1.8226),\n",
       "  tensor(1.8626),\n",
       "  tensor(1.8451),\n",
       "  tensor(1.7861),\n",
       "  tensor(1.7135),\n",
       "  tensor(1.8110),\n",
       "  tensor(1.7635),\n",
       "  tensor(1.7572),\n",
       "  tensor(1.7369),\n",
       "  tensor(1.8163),\n",
       "  tensor(1.8079),\n",
       "  tensor(1.8025),\n",
       "  tensor(1.8126),\n",
       "  tensor(1.6967),\n",
       "  tensor(1.7586),\n",
       "  tensor(1.6757),\n",
       "  tensor(1.7017),\n",
       "  tensor(1.7322),\n",
       "  tensor(1.7760),\n",
       "  tensor(1.7703),\n",
       "  tensor(1.7344),\n",
       "  tensor(1.7475),\n",
       "  tensor(1.7288),\n",
       "  tensor(1.7562),\n",
       "  tensor(1.7172),\n",
       "  tensor(1.7636),\n",
       "  tensor(1.7166),\n",
       "  tensor(1.7014),\n",
       "  tensor(1.7692),\n",
       "  tensor(1.6890),\n",
       "  tensor(1.8023),\n",
       "  tensor(1.6207),\n",
       "  tensor(1.6356),\n",
       "  tensor(1.6613),\n",
       "  tensor(1.6893),\n",
       "  tensor(1.6776),\n",
       "  tensor(1.7136),\n",
       "  tensor(1.7258),\n",
       "  tensor(1.7152),\n",
       "  tensor(1.6588),\n",
       "  tensor(1.6883),\n",
       "  tensor(1.6349),\n",
       "  tensor(1.6581),\n",
       "  tensor(1.5743),\n",
       "  tensor(1.5686),\n",
       "  tensor(1.6403),\n",
       "  tensor(1.7338),\n",
       "  tensor(1.6781),\n",
       "  tensor(1.5795),\n",
       "  tensor(1.6038),\n",
       "  tensor(1.6339),\n",
       "  tensor(1.5687),\n",
       "  tensor(1.5267),\n",
       "  tensor(1.5222),\n",
       "  tensor(1.5250),\n",
       "  tensor(1.6211),\n",
       "  tensor(1.6017),\n",
       "  tensor(1.5878),\n",
       "  tensor(1.5324),\n",
       "  tensor(1.6514),\n",
       "  tensor(1.5443),\n",
       "  tensor(1.6215),\n",
       "  tensor(1.4927),\n",
       "  tensor(1.5045),\n",
       "  tensor(1.4927),\n",
       "  tensor(1.5753),\n",
       "  tensor(1.5837),\n",
       "  tensor(1.6135),\n",
       "  tensor(1.4475),\n",
       "  tensor(1.5009),\n",
       "  tensor(1.4399),\n",
       "  tensor(1.5058),\n",
       "  tensor(1.5371),\n",
       "  tensor(1.4212),\n",
       "  tensor(1.4780),\n",
       "  tensor(1.4889),\n",
       "  tensor(1.3793),\n",
       "  tensor(1.5152),\n",
       "  tensor(1.4843),\n",
       "  tensor(1.4989),\n",
       "  tensor(1.4568),\n",
       "  tensor(1.4251),\n",
       "  tensor(1.4207),\n",
       "  tensor(1.3020),\n",
       "  tensor(1.5372),\n",
       "  tensor(1.5435),\n",
       "  tensor(1.4037),\n",
       "  tensor(1.4087),\n",
       "  tensor(1.5488),\n",
       "  tensor(1.4912),\n",
       "  tensor(1.3875),\n",
       "  tensor(1.3720),\n",
       "  tensor(1.3047),\n",
       "  tensor(1.4545),\n",
       "  tensor(1.2872),\n",
       "  tensor(1.4068),\n",
       "  tensor(1.2377),\n",
       "  tensor(1.3925),\n",
       "  tensor(1.3090),\n",
       "  tensor(1.3805),\n",
       "  tensor(1.3424),\n",
       "  tensor(1.3795),\n",
       "  tensor(1.4243),\n",
       "  tensor(1.3610),\n",
       "  tensor(1.4078),\n",
       "  tensor(1.2887),\n",
       "  tensor(1.3908),\n",
       "  tensor(1.3251),\n",
       "  tensor(1.2606),\n",
       "  tensor(1.1425),\n",
       "  tensor(1.4070),\n",
       "  tensor(1.2104),\n",
       "  tensor(1.3625),\n",
       "  tensor(1.3538),\n",
       "  tensor(1.3032),\n",
       "  tensor(1.2813),\n",
       "  tensor(1.3512),\n",
       "  tensor(1.2379),\n",
       "  tensor(1.2744),\n",
       "  tensor(1.1590),\n",
       "  tensor(1.1803),\n",
       "  tensor(1.2598),\n",
       "  tensor(1.2314),\n",
       "  tensor(1.2771),\n",
       "  tensor(1.1906),\n",
       "  tensor(1.1759),\n",
       "  tensor(1.2306),\n",
       "  tensor(1.2563),\n",
       "  tensor(1.0670),\n",
       "  tensor(1.1467),\n",
       "  tensor(1.1871),\n",
       "  tensor(1.1378),\n",
       "  tensor(1.3263),\n",
       "  tensor(1.2230),\n",
       "  tensor(1.1610),\n",
       "  tensor(1.1807),\n",
       "  tensor(1.1971),\n",
       "  tensor(1.1652),\n",
       "  tensor(1.3294),\n",
       "  tensor(1.2215),\n",
       "  tensor(0.9707),\n",
       "  tensor(1.1026),\n",
       "  tensor(1.0708),\n",
       "  tensor(1.0660),\n",
       "  tensor(1.2279),\n",
       "  tensor(0.9279),\n",
       "  tensor(1.2345),\n",
       "  tensor(0.9563),\n",
       "  tensor(1.1257),\n",
       "  tensor(1.0841),\n",
       "  tensor(0.9838),\n",
       "  tensor(0.9322),\n",
       "  tensor(1.2008),\n",
       "  tensor(1.1313),\n",
       "  tensor(1.0890),\n",
       "  tensor(0.9294),\n",
       "  tensor(0.9780),\n",
       "  tensor(1.2057),\n",
       "  tensor(1.0462),\n",
       "  tensor(1.1404),\n",
       "  tensor(1.1563),\n",
       "  tensor(0.9489),\n",
       "  tensor(1.1293),\n",
       "  tensor(1.0523),\n",
       "  tensor(0.9861),\n",
       "  tensor(1.0649),\n",
       "  tensor(0.8510),\n",
       "  tensor(1.0140),\n",
       "  tensor(0.9715),\n",
       "  tensor(0.8318),\n",
       "  tensor(1.0523),\n",
       "  tensor(1.0183),\n",
       "  tensor(0.9041),\n",
       "  tensor(1.1314),\n",
       "  tensor(0.9907),\n",
       "  tensor(0.9298),\n",
       "  tensor(1.0347),\n",
       "  tensor(1.1252),\n",
       "  tensor(0.9320),\n",
       "  tensor(0.9670),\n",
       "  tensor(1.1057),\n",
       "  tensor(0.9845),\n",
       "  tensor(1.0778),\n",
       "  tensor(0.9211),\n",
       "  tensor(0.9160),\n",
       "  tensor(0.9005),\n",
       "  tensor(0.8541),\n",
       "  tensor(0.8209),\n",
       "  tensor(1.0488),\n",
       "  tensor(0.8703),\n",
       "  tensor(0.9466),\n",
       "  tensor(0.8812),\n",
       "  tensor(0.8470),\n",
       "  tensor(1.0183),\n",
       "  tensor(0.8910),\n",
       "  tensor(0.9104),\n",
       "  tensor(0.9610),\n",
       "  tensor(0.9042),\n",
       "  tensor(0.8569),\n",
       "  tensor(0.9088),\n",
       "  tensor(0.8066),\n",
       "  tensor(0.8078),\n",
       "  tensor(0.8876),\n",
       "  tensor(0.9733),\n",
       "  tensor(0.8762),\n",
       "  tensor(0.8123),\n",
       "  tensor(0.8955),\n",
       "  tensor(0.9302),\n",
       "  tensor(0.9966),\n",
       "  tensor(0.7034),\n",
       "  tensor(0.9977),\n",
       "  tensor(0.9206),\n",
       "  tensor(1.0167),\n",
       "  tensor(0.8256),\n",
       "  tensor(0.9794),\n",
       "  tensor(1.0252),\n",
       "  tensor(1.0256),\n",
       "  tensor(0.9030),\n",
       "  tensor(0.7961),\n",
       "  tensor(0.8232),\n",
       "  tensor(0.8670),\n",
       "  tensor(0.7599),\n",
       "  tensor(0.8566),\n",
       "  tensor(0.9228),\n",
       "  tensor(0.9394),\n",
       "  tensor(0.9453),\n",
       "  tensor(0.7122),\n",
       "  tensor(0.9767),\n",
       "  tensor(0.7468),\n",
       "  tensor(0.6894),\n",
       "  tensor(0.8645),\n",
       "  tensor(0.9369),\n",
       "  tensor(0.7043),\n",
       "  tensor(0.9124),\n",
       "  tensor(0.8621),\n",
       "  tensor(0.8050),\n",
       "  tensor(0.7377),\n",
       "  tensor(0.7789),\n",
       "  tensor(0.6879),\n",
       "  tensor(0.8489),\n",
       "  tensor(0.8891),\n",
       "  tensor(0.8154),\n",
       "  tensor(0.9087),\n",
       "  tensor(0.7271),\n",
       "  tensor(0.7767),\n",
       "  tensor(0.8050),\n",
       "  tensor(0.8535),\n",
       "  tensor(0.8115),\n",
       "  tensor(0.7652),\n",
       "  tensor(0.8928),\n",
       "  tensor(1.0249),\n",
       "  tensor(0.7057),\n",
       "  tensor(0.6866),\n",
       "  tensor(0.6282),\n",
       "  tensor(0.9207),\n",
       "  tensor(0.7369),\n",
       "  tensor(0.7385),\n",
       "  tensor(0.7148),\n",
       "  tensor(0.8486),\n",
       "  tensor(0.6561),\n",
       "  tensor(0.9273),\n",
       "  tensor(0.8051),\n",
       "  tensor(0.8075),\n",
       "  tensor(0.6617),\n",
       "  tensor(0.7898),\n",
       "  tensor(0.6231),\n",
       "  tensor(0.9199),\n",
       "  tensor(0.8302),\n",
       "  tensor(0.8879),\n",
       "  tensor(0.5562),\n",
       "  tensor(0.8272),\n",
       "  tensor(0.8500),\n",
       "  tensor(0.5387),\n",
       "  tensor(0.7785),\n",
       "  tensor(1.0330),\n",
       "  tensor(0.7356),\n",
       "  tensor(0.7388),\n",
       "  tensor(0.6851),\n",
       "  tensor(0.8252),\n",
       "  tensor(0.7138),\n",
       "  tensor(0.6257),\n",
       "  tensor(0.6030),\n",
       "  tensor(0.6884),\n",
       "  tensor(0.5794),\n",
       "  tensor(0.6739),\n",
       "  tensor(0.5972),\n",
       "  tensor(0.5876),\n",
       "  tensor(0.7341),\n",
       "  tensor(0.6752),\n",
       "  tensor(0.8359),\n",
       "  tensor(0.7357),\n",
       "  tensor(0.6605),\n",
       "  tensor(0.5694),\n",
       "  tensor(0.7708),\n",
       "  tensor(0.7994),\n",
       "  tensor(0.5980),\n",
       "  tensor(0.7718),\n",
       "  tensor(0.7250),\n",
       "  tensor(0.6990),\n",
       "  tensor(0.7285),\n",
       "  tensor(0.7091),\n",
       "  tensor(0.6955),\n",
       "  ...])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyMNISTNet = MNISTNet()\n",
    "#MyMNISTNet.cuda() #<= if cuda\n",
    "\n",
    "MNISTOptimizer = createOptimizer(MyMNISTNet)\n",
    "\n",
    "train(MyMNISTNet, train_loader, MNISTOptimizer, count_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input has less dimensions than expected",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-773f8e08b173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#print(Variable(evaluate_x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyMNISTNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-0008d5c8dd39>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#conv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input has less dimensions than expected"
     ]
    }
   ],
   "source": [
    "#evaluate_x = Variable(test_loader.dataset.test_data.type_as(torch.FloatTensor()))\n",
    "#evaluate_y = Variable(test_loader.dataset.test_labels)\n",
    "\n",
    "evaluate_x = test_loader.dataset.test_data[0]\n",
    "#evaluate_x = Variable(evaluate_x)\n",
    "\n",
    "#evaluate_y = Variable(test_loader.dataset.test_labels[0])\n",
    "\n",
    "#print(Variable(evaluate_x))\n",
    "output = MyMNISTNet(evaluate_x)\n",
    "\n",
    "print(output)\n",
    "\n",
    "#pred = output.data.max(1)[1]\n",
    "#d = pred.eq(evaluate_y.data).cpu()\n",
    "#accuracy = d.sum()/d.size()[0]\n",
    "\n",
    "#print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAAxtJREFUeJztmz1LI1EUhp9ZVrQI8YMsSMBsQEEREUEbbQQVieIPEPwHFnbaKUgaQbFSUPQnmEIQsREELQIKYmMXktgIrh+FhSDK3SJrsp4UmZHczCx7HgiBy2XuyZP3JHMnE8cYg1Lim98FBA0VIlAhAhUiUCECFSJQIQIVIlAhgu9eJkciEROPxy2VYpdcLsf9/b1TaZ4nIfF4nIuLi69X5SMDAwOu5mnLCFSIQIUIVIhAhQhUiECFCFSIQIUIPJ2pumVvbw+AnZ0dAKLRKA0NDQDMzMwA0NraCkBHR4eNEr6MJkRgJSHz8/NAYUMl2draAiAcDgPQ3d3t+fhtbW0ALCwsAO73KW6wImR3dxeAq6sroPCir6+vAbi8vATg5OQEgHQ6TSwWA+Dm5qbsWHV1dQBEIhEAbm9vSafTQElMNYVoywisJGR0dPTTM0Aikfg05+npCSgk5uMdPj8/LztWfX09AJ2dnQB0dXXx+PgIQHt7e5Ur14SUYSUhbmhubgZgZGSkOPZ3oiSpVAooJKu3txeA6enpqtelCRH4lhC33N3dATA7OwuAMYalpSUAWlpaqr5e4IVsbm4CJTFNTU3FD1gbaMsIApuQs7MzAFZWVj6N7+/v09PTY21dTYggsAk5PDwE4PX1FYCxsTEABgcHra4bSCEvLy8cHR0BpTPV5eVloLS3sYW2jCCQCVldXS3uiicmJgAYGhqqydqaEEGgEnJwcABAMpmksbERgMXFxZrWoAkRBCIhDw8PAMzNzQHw9vbG5OQkYP9rVuK7kPf39+LFo2w2CxSuxCeTSV/q0ZYR+J6QTCZTdlfS+vq6lcuDbtCECHxLSD6fB2B8fLw4tra2BsDU1JQvNYGPQra3t4GSGIDh4WEAHKfizYLW0JYR1Dwhp6enAGxsbNR6aVdoQgQ1T8jHpcHn5+fi2MctEaFQqNbllKEJEfh+YtbX18fx8TFg53cWzxhjXD/6+/vNv8qf2iu+Rm0ZgWM8/JHZcZxfQL7ixGDy0xjzo9IkT0L+B7RlBCpEoEIEKkSgQgQqRKBCBCpEoEIEvwEKw6jrEKgkZwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAA11JREFUeJztmz1LI1EUhp/o2ogiSEKwMQEbBRFBIQiCliIqouAUIhZW/gURCxHx20rwB1j4hYrYCNYWEhAtFCSIW4lsgliJWswWbibkTPyYxbnjsueBEDj3hDnz5j137p1JQrZto+QoCrqA74YKIlBBBCqIQAURqCACFUSggghUEMEPL8nhcNiOx+M+leIvNzc3pNPp0Ed5ngSJx+Mkk8m/rypAmpubP5WnLSNQQQSeWuYrWFhYAODx8RGA8/Nztre383JGR0dpaWkBYGhoyGh96hCBMYdYlgXA1taWaywUyp/8V1dXOTo6AqCtrQ2A6upqnyt8RR0iMOIQy7IKOgOgtraWjo4OAK6vrwHY398nlUoBsLa2BsDY2JiBSn0WJLtm2d3ddWL19fXA60kDhMNhysrKAHh+fgYgkUhwdnYGQCaT8bNEF9oyAl8dcnt7C4Bt244zDg8PAaiqqnLlZy/Jl5eXTqyrq8vPEl2oQwS+OqS7uxuAVCpFeXk5AJWVlW/mb2xsALm5JAiMXGVisdi74/Pz8wBcXV05sUQikfduCm0ZgfG9jOTg4ICJiQkAnp6eAIhGo8zMzABQWlpqtB51iCBwhySTSccZWSzLcvYwplGHCAJzSG9vL5BbqAEMDw8DMDU1FUhNEIAg2dXr8fEx8DqRRiIRAMbHxwGcvU0QaMsIjDukr68PgHQ67cQGBwcBqKmpMV2OC3WIwJhDsvc/Tk9P8+Lt7e1MTk6aKuNDjAiSyWSYnp4G3Bu3xsbGQCdRibaMwIhDFhcXOTk5yYtl1yHfqV1AHeLCiEOWlpZcsZWVFSDYRVgh1CGCwPYy2ccLJSUlBccrKiryxl9eXgB4eHhwcu7v7wFYXl52fb64uBiA2dlZT/dUAhOkoaHh3fGBgQEgd3f+7u4OgPX1dU/HiUajzh7pM2jLCIw4pLOzk729PU+f2dzcfHMs20ZFRbnvs6enB3D/Uqi1tdXTcdUhAiMO2dnZYW5uDij8zOXi4gIoPD+MjIwA+Y8y+vv7Aairq/vyWrFt+9OvpqYm+1/lT+0fnqO2jEAFEaggAhVEoIIIVBCBCiJQQQQh28MfmUOh0C/gp3/l+ErMtu3IR0meBPkf0JYRqCACFUSggghUEIEKIlBBBCqIQAUR/AZTesie1UWqVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(test_loader.dataset.test_data)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_images_separately(images):\n",
    "    \"Plot the six MNIST images separately.\"\n",
    "    fig = plt.figure()\n",
    "    for j in range(1, 7):\n",
    "        ax = fig.add_subplot(1, 6, j)\n",
    "        ax.matshow(images[j-1], cmap = matplotlib.cm.binary)\n",
    "        plt.xticks(np.array([]))\n",
    "        plt.yticks(np.array([]))\n",
    "    plt.show()\n",
    "\n",
    "def plot_image(image):\n",
    "    \"Plot a MNIST image.\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(4, 1, 1)\n",
    "    ax.matshow(image, cmap = matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "    plt.show()\n",
    "    \n",
    "plot_images_separately(test_loader.dataset.test_data)\n",
    "plot_images_separately([\n",
    "    test_loader.dataset.test_data[0],\n",
    "    test_loader.dataset.test_data[1],\n",
    "    test_loader.dataset.test_data[1],\n",
    "    test_loader.dataset.test_data[1],\n",
    "    test_loader.dataset.test_data[1],\n",
    "    test_loader.dataset.test_data[1],\n",
    "    test_loader.dataset.test_data[1],\n",
    "    test_loader.dataset.test_data[1]\n",
    "])\n",
    "plot_image(test_loader.dataset.test_data[0])\n",
    "plot_image(test_loader.dataset.test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
